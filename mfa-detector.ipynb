{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6198357,"sourceType":"datasetVersion","datasetId":3558442},{"sourceId":6198381,"sourceType":"datasetVersion","datasetId":3558462},{"sourceId":6198400,"sourceType":"datasetVersion","datasetId":3558476}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-01T16:58:01.001110Z","iopub.execute_input":"2023-08-01T16:58:01.001423Z","iopub.status.idle":"2023-08-01T16:58:01.026933Z","shell.execute_reply.started":"2023-08-01T16:58:01.001375Z","shell.execute_reply":"2023-08-01T16:58:01.025922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# M.F.A Site Detector","metadata":{}},{"cell_type":"markdown","source":"  Recently an article was published in Forbes stating that around 20% of all programmatic media spend is going to 'Made For Advertising' (MFA) sites. These are sites that generate money by driving user visits through clicks on adverts placed cheaply on social media sites or at the bottom of genuine news articles, these visits are then turned into advertising revenue on their site. The content on MFA sites is generally complete rubbish: fake celeberity gossip, phony deals on expensive clothes, seemingly lucrative financial advice... All designed to drive traffic to their site, no matter how fleeting. \n\n  MFA owners have cleverly made their sites very attractrive places to advertise on, at least on the surface; high viewability , impressive click rates and low CPMs mean that when automatic processes are deployed to optimise digital marketing campaigns, large sums of money are directed towards them, so much so that MFA is now a multi-billion dollar industry. \n\n  The complete lack of journalistic integrity and the fact that it is now considered an embarsement to have your brand name appear on one of these sites means that advertisers have recently decided that any and all brand activity on these sites should cease. Digital marketers have reacted quickly by blocking large lists of sites with low proportions of organic visits (people visiting the site by simply searching it) compared to those driven there by clicks. For the time being this has kept the MFA sites at bay, however inevitably there will be ways found around this, especially with a multi-billion dolar industy at stake. \n\n  Whether it's the equivuilant of click farms in Asia , or bots on ever moving VPN adresses automating the process, ways will be found to forge organic visits, rendering the ratio mentioned above useless. However what wont change about these sites is the content. The way they keep their costs down is by generating countless, cheap, clickbait articles and then bombarding you with ads before you get the chance to leave the page. A human can take one look at these sites and tell what it is. But can a machine? Lets find out...","metadata":{}},{"cell_type":"markdown","source":"#  Gathering Data","metadata":{}},{"cell_type":"markdown","source":"I have a list of 1600 or so MFA sites, generated simply by looking for suspicious ratios of organic to click driven traffic. A similar length list is generated of reputable sites. I erred on the side of caution here and went for sites where impressions are more costly, assuming that the digital marketing world has collectivley got it right (bold) and is bidding more for higher quality inventory. I hope this does not simply mean any model I make will simply be looking for evidence of more expensive, upper crust sites, however this may not be a bad start!","metadata":{}},{"cell_type":"markdown","source":"I then use selenium to iterate through these lists and scrape anything and everything it can from each site. Let's import the data and see what we've got!\n\n\n","metadata":{}},{"cell_type":"code","source":"mfa_docs=pd.read_csv(r\"/kaggle/input/mfa-docs/MFA_documents.csv\")\nmfa_docs.drop(\"Unnamed: 0\", axis=1, inplace=True)\nmfa_docs[\"MFA\"]=1\nmfa_docs.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:48:10.826143Z","iopub.execute_input":"2023-08-01T17:48:10.827440Z","iopub.status.idle":"2023-08-01T17:48:10.965402Z","shell.execute_reply.started":"2023-08-01T17:48:10.827391Z","shell.execute_reply":"2023-08-01T17:48:10.964212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Had to change encoding from the default UTF-8 in the Non MFA docs as there are some seriously dodgy characters in there, l will need to remove rows containing anything special before tokenisation\n\nIve changed MFA from True Flase here to 1/0 in the hopes it may help, I suspect the model wasnt rigged for boolean.\n\n","metadata":{}},{"cell_type":"code","source":"non_mfa_docs=pd.read_csv(r\"/kaggle/input/d/lawrencebutler/non-mfa-docs/Non_MFA_ Documents.csv\", encoding='Latin-1')\nnon_mfa_docs[\"MFA\"]=0\nnon_mfa_docs.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:48:13.237896Z","iopub.execute_input":"2023-08-01T17:48:13.238289Z","iopub.status.idle":"2023-08-01T17:48:13.313450Z","shell.execute_reply.started":"2023-08-01T17:48:13.238250Z","shell.execute_reply":"2023-08-01T17:48:13.312309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now lets try and get rid of dodgy characters\n","metadata":{}},{"cell_type":"code","source":"non_mfa_docs[\"Document\"]=non_mfa_docs[\"Document\"].str.replace('\\W',' ', regex=True)\nnon_mfa_docs.describe()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:48:14.300900Z","iopub.execute_input":"2023-08-01T17:48:14.301349Z","iopub.status.idle":"2023-08-01T17:48:14.585161Z","shell.execute_reply.started":"2023-08-01T17:48:14.301315Z","shell.execute_reply":"2023-08-01T17:48:14.583931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mfa_docs[\"Document\"]=mfa_docs[\"Document\"].str.replace('\\W',' ', regex=True)\nmfa_docs.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:48:14.891495Z","iopub.execute_input":"2023-08-01T17:48:14.891902Z","iopub.status.idle":"2023-08-01T17:48:15.324405Z","shell.execute_reply.started":"2023-08-01T17:48:14.891870Z","shell.execute_reply":"2023-08-01T17:48:15.323169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"still the occasional dodgy character or accented letter (ø for example) , but lets roll with it for now, hoping that in tokenisation i can just ignore any errors thrown, including foreign/arabic stuff.\n\nlets take a sample of each and try tokenising","metadata":{}},{"cell_type":"code","source":"df1=mfa_docs.sample(300 , random_state = 42)\ndf2=non_mfa_docs.sample(300 , random_state=42)\nrandom_samples=[df1,df2]\ndf=pd.concat(random_samples)\ndf.rename({'MFA': 'labels'}, axis=1, inplace=True)\ndf['labels'] = df['labels'].astype(float) \ndf['Document'] = df['Document'].str[:3000] # a little over 3000 here seems to be the limit, 4000 = no dice\ndf.head()\neval_df=df.sample(50 , random_state = 42)\neval_df.head()\ndf= pd.concat([df , eval_df]).drop_duplicates(keep=False)\ndf.dropna(inplace=True)\neval_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:25.403470Z","iopub.execute_input":"2023-08-01T17:50:25.404405Z","iopub.status.idle":"2023-08-01T17:50:25.432283Z","shell.execute_reply.started":"2023-08-01T17:50:25.404357Z","shell.execute_reply":"2023-08-01T17:50:25.431256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"above i have limited the character count of the document due memory limit troubles in training\n\nlets try investigating how much we need to cut it down by","metadata":{}},{"cell_type":"code","source":"avg_word_length=5.7\n(sum(df['Document'].str.len())/df.shape[0])#/avg_word_length\n","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:25.845382Z","iopub.execute_input":"2023-08-01T17:50:25.846522Z","iopub.status.idle":"2023-08-01T17:50:25.855532Z","shell.execute_reply.started":"2023-08-01T17:50:25.846476Z","shell.execute_reply":"2023-08-01T17:50:25.854230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so about 4.5k characters per document on average, equating to about 800 words, lets cut this down by 75 %. It seems around 3000 characters is the limit for the model im using here.","metadata":{}},{"cell_type":"markdown","source":"# Tokenisation","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset,DatasetDict\nds=Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:26.613317Z","iopub.execute_input":"2023-08-01T17:50:26.613972Z","iopub.status.idle":"2023-08-01T17:50:26.629429Z","shell.execute_reply.started":"2023-08-01T17:50:26.613938Z","shell.execute_reply":"2023-08-01T17:50:26.628170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:26.869701Z","iopub.execute_input":"2023-08-01T17:50:26.870688Z","iopub.status.idle":"2023-08-01T17:50:26.878561Z","shell.execute_reply.started":"2023-08-01T17:50:26.870638Z","shell.execute_reply":"2023-08-01T17:50:26.877370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_nm = 'microsoft/deberta-v3-small'\n","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:27.224076Z","iopub.execute_input":"2023-08-01T17:50:27.225263Z","iopub.status.idle":"2023-08-01T17:50:27.230656Z","shell.execute_reply.started":"2023-08-01T17:50:27.225180Z","shell.execute_reply":"2023-08-01T17:50:27.229150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm , padding= 'max_length' , truncation = 'max_length')","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:27.628509Z","iopub.execute_input":"2023-08-01T17:50:27.628882Z","iopub.status.idle":"2023-08-01T17:50:28.800045Z","shell.execute_reply.started":"2023-08-01T17:50:27.628849Z","shell.execute_reply":"2023-08-01T17:50:28.798695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tok_func(x): return tokz(x[\"Document\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:28.802339Z","iopub.execute_input":"2023-08-01T17:50:28.802731Z","iopub.status.idle":"2023-08-01T17:50:28.808177Z","shell.execute_reply.started":"2023-08-01T17:50:28.802695Z","shell.execute_reply":"2023-08-01T17:50:28.806760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_ds = ds.map(tok_func, batched=True )","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:28.810300Z","iopub.execute_input":"2023-08-01T17:50:28.810722Z","iopub.status.idle":"2023-08-01T17:50:29.691089Z","shell.execute_reply.started":"2023-08-01T17:50:28.810675Z","shell.execute_reply":"2023-08-01T17:50:29.690082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_ds.remove_columns([\"__index_level_0__\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:29.693139Z","iopub.execute_input":"2023-08-01T17:50:29.693861Z","iopub.status.idle":"2023-08-01T17:50:29.704848Z","shell.execute_reply.started":"2023-08-01T17:50:29.693821Z","shell.execute_reply":"2023-08-01T17:50:29.703381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"i was thrown here as i was typing _ (normal underscore) when infact a ▁ (long underscore) was needed U+FF3F","metadata":{}},{"cell_type":"code","source":"#row#[\"Document\"]#,row[\"input_ids\"]\n#just checking tokens line up! they do!","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:30.720519Z","iopub.execute_input":"2023-08-01T17:50:30.720986Z","iopub.status.idle":"2023-08-01T17:50:30.725091Z","shell.execute_reply.started":"2023-08-01T17:50:30.720950Z","shell.execute_reply":"2023-08-01T17:50:30.724244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting training and validation sets , creating test set\n","metadata":{}},{"cell_type":"markdown","source":"Transformers uses a DatasetDict for holding your training and validation sets. To create one that contains 25% of our data for the validation set, and 75% for the training set, use train_test_split\n\n\nbe careful here, confusingly here they've valled the vaildation set the test set!\n","metadata":{}},{"cell_type":"code","source":"#tok_ds.rename_column( 'MFA' , 'labels')\ndds=tok_ds.train_test_split(0.25 , seed=42)\ndds","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:33.239912Z","iopub.execute_input":"2023-08-01T17:50:33.240371Z","iopub.status.idle":"2023-08-01T17:50:33.256405Z","shell.execute_reply.started":"2023-08-01T17:50:33.240333Z","shell.execute_reply":"2023-08-01T17:50:33.255392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I should have tokenised everything and then split up into smaller sets so i could scale easily later, i'll do this now , DONE (WITH LARGER SET CAME SOME GARBAGE, DROPPED NA)\n\n","metadata":{}},{"cell_type":"code","source":"eval_ds=Dataset.from_pandas(eval_df).map(tok_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:34.637979Z","iopub.execute_input":"2023-08-01T17:50:34.638374Z","iopub.status.idle":"2023-08-01T17:50:34.767285Z","shell.execute_reply.started":"2023-08-01T17:50:34.638338Z","shell.execute_reply":"2023-08-01T17:50:34.766168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Time!","metadata":{}},{"cell_type":"code","source":"import numpy\nfrom transformers import TrainingArguments , Trainer","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:50:36.075458Z","iopub.execute_input":"2023-08-01T17:50:36.075838Z","iopub.status.idle":"2023-08-01T17:50:36.083344Z","shell.execute_reply.started":"2023-08-01T17:50:36.075805Z","shell.execute_reply":"2023-08-01T17:50:36.082293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bs = 2\nepochs = 4","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:51:41.415747Z","iopub.execute_input":"2023-08-01T17:51:41.416108Z","iopub.status.idle":"2023-08-01T17:51:41.421405Z","shell.execute_reply.started":"2023-08-01T17:51:41.416076Z","shell.execute_reply":"2023-08-01T17:51:41.420066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=8e-5","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:51:42.527619Z","iopub.execute_input":"2023-08-01T17:51:42.528025Z","iopub.status.idle":"2023-08-01T17:51:42.533926Z","shell.execute_reply.started":"2023-08-01T17:51:42.527992Z","shell.execute_reply":"2023-08-01T17:51:42.532237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def corr(x,y): return np.corrcoef(x,y)[0][1]\ndef corr_d(eval_pred): return {'pearson': corr(*eval_pred)}","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:51:43.315095Z","iopub.execute_input":"2023-08-01T17:51:43.315839Z","iopub.status.idle":"2023-08-01T17:51:43.321477Z","shell.execute_reply.started":"2023-08-01T17:51:43.315804Z","shell.execute_reply":"2023-08-01T17:51:43.320137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:51:43.827123Z","iopub.execute_input":"2023-08-01T17:51:43.827991Z","iopub.status.idle":"2023-08-01T17:51:43.834497Z","shell.execute_reply.started":"2023-08-01T17:51:43.827951Z","shell.execute_reply":"2023-08-01T17:51:43.833174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=corr_d )","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:51:44.429008Z","iopub.execute_input":"2023-08-01T17:51:44.429407Z","iopub.status.idle":"2023-08-01T17:51:46.184313Z","shell.execute_reply.started":"2023-08-01T17:51:44.429372Z","shell.execute_reply":"2023-08-01T17:51:46.183153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train();","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:51:46.186860Z","iopub.execute_input":"2023-08-01T17:51:46.187297Z","iopub.status.idle":"2023-08-01T17:54:37.725475Z","shell.execute_reply.started":"2023-08-01T17:51:46.187256Z","shell.execute_reply":"2023-08-01T17:54:37.724460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = trainer.predict(eval_ds).predictions.astype(float)\npreds","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:54:57.934854Z","iopub.execute_input":"2023-08-01T17:54:57.935254Z","iopub.status.idle":"2023-08-01T17:54:59.514146Z","shell.execute_reply.started":"2023-08-01T17:54:57.935193Z","shell.execute_reply":"2023-08-01T17:54:59.513067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.clip(preds, 0, 1)\npreds","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:55:00.385953Z","iopub.execute_input":"2023-08-01T17:55:00.386352Z","iopub.status.idle":"2023-08-01T17:55:00.394667Z","shell.execute_reply.started":"2023-08-01T17:55:00.386317Z","shell.execute_reply":"2023-08-01T17:55:00.393589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_df","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:55:02.749034Z","iopub.execute_input":"2023-08-01T17:55:02.749859Z","iopub.status.idle":"2023-08-01T17:55:02.777055Z","shell.execute_reply.started":"2023-08-01T17:55:02.749811Z","shell.execute_reply":"2023-08-01T17:55:02.775885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_df['labels'] = eval_df['labels'].astype(float)\nincorrect_percent=sum((preds.round()-eval_df['labels'])**2)/100\nincorrect_percent\n","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:55:08.096494Z","iopub.execute_input":"2023-08-01T17:55:08.096879Z","iopub.status.idle":"2023-08-01T17:55:08.106214Z","shell.execute_reply.started":"2023-08-01T17:55:08.096846Z","shell.execute_reply":"2023-08-01T17:55:08.104973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"17% error is the best ive got, not terrible, better than a purely random guess by a factor of 3\n\n\nimproved  to 15% by cleaning the rubbish from the documents ","metadata":{}},{"cell_type":"code","source":"preds.round()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:56:16.507280Z","iopub.execute_input":"2023-08-01T17:56:16.507653Z","iopub.status.idle":"2023-08-01T17:56:16.515462Z","shell.execute_reply.started":"2023-08-01T17:56:16.507622Z","shell.execute_reply":"2023-08-01T17:56:16.514398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_df[\"labels\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-01T17:56:17.077881Z","iopub.execute_input":"2023-08-01T17:56:17.079165Z","iopub.status.idle":"2023-08-01T17:56:17.093659Z","shell.execute_reply.started":"2023-08-01T17:56:17.079114Z","shell.execute_reply":"2023-08-01T17:56:17.092289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Improvements to be made","metadata":{}},{"cell_type":"markdown","source":"15% error isn't bad, especially considering the small data set and the blurred line between the two types of site im choosing between here.\n\nKey changes that I suspect will improve the accuracy further. \n* ensemble the model with others that take into account the ads.txt data from the site\n* change the base model used for one that already has weights trained on website classification","metadata":{}}]}